{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    6     3    12\n",
      "    7     1     8\n",
      "    2    14     3\n",
      "   16     2     5\n",
      "   11     9    10\n",
      "    2    13     2\n",
      "   15     4     0\n",
      "    2     2     0\n",
      "   10    15     0\n",
      "    2     0     0\n",
      "[torch.LongTensor of size 10x3]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0902  0.0142  0.0054 -0.1126  0.0394\n",
      " -0.0251  0.0552  0.0039  0.0441 -0.0241\n",
      " -0.0811 -0.1490 -0.1966  0.1724  0.1074\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.0223  0.0563  0.1778 -0.1500  0.0378\n",
      "  0.2266 -0.1383  0.1943 -0.1969 -0.1424\n",
      " -0.0822  0.0351  0.0002  0.0687  0.0513\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.0147  0.1148 -0.1016 -0.2115  0.1299\n",
      "  0.1662  0.0198 -0.0919 -0.0830 -0.0050\n",
      " -0.1020  0.0612  0.0117  0.1067  0.0106\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.0399  0.0958  0.1701 -0.0751 -0.1548\n",
      "  0.0436  0.0814 -0.3451 -0.2643  0.0989\n",
      " -0.0820  0.0803 -0.2821 -0.1690  0.0578\n",
      "\n",
      "(4 ,.,.) = \n",
      "  0.0523 -0.1280  0.3578 -0.3141 -0.1152\n",
      "  0.0697  0.1416 -0.1687 -0.2225  0.0639\n",
      " -0.2033  0.0992 -0.3016 -0.0572  0.1865\n",
      "\n",
      "(5 ,.,.) = \n",
      " -0.0141  0.0551 -0.0620 -0.2223  0.0322\n",
      " -0.0216  0.0669 -0.3446 -0.4308  0.2252\n",
      " -0.1641  0.1122 -0.3296 -0.1179  0.1781\n",
      "\n",
      "(6 ,.,.) = \n",
      "  0.0045  0.1092 -0.2102 -0.0506  0.0949\n",
      "  0.0658 -0.1512 -0.1867 -0.4167  0.0077\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "(7 ,.,.) = \n",
      " -0.0240  0.1187 -0.3340 -0.1683  0.1519\n",
      " -0.0223  0.0061 -0.3259 -0.3020  0.1313\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "(8 ,.,.) = \n",
      " -0.0974  0.0953 -0.3648  0.0130  0.2358\n",
      " -0.0161  0.0935 -0.3553 -0.0429  0.1443\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "(9 ,.,.) = \n",
      " -0.0894  0.1109 -0.3614 -0.0767  0.1862\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 10x3x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "seqs = ['ghatmasala','nicela','c-pakodas']\n",
    "\n",
    "# make <pad> idx 0\n",
    "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n",
    "\n",
    "# make model\n",
    "embed = nn.Embedding(len(vocab), 10)\n",
    "lstm = nn.LSTM(10, 5)\n",
    "\n",
    "vectorized_seqs = [[vocab.index(tok) for tok in seq] for seq in seqs]\n",
    "\n",
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
    "\n",
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "\n",
    "# SORT YOUR TENSORS BY LENGTH!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
    "# Otherwise, give (L,B,D) tensors\n",
    "seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "print(seq_tensor)\n",
    "# embed your sequences\n",
    "seq_tensor = embed(seq_tensor)\n",
    "\n",
    "# pack them up nicely\n",
    "packed_input = pack_padded_sequence(seq_tensor, seq_lengths.cpu().numpy())\n",
    "\n",
    "# throw them through your LSTM (remember to give batch_first=True here if you packed with it)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "\n",
    "# unpack your output if required\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "print(output)\n",
    "\n",
    "# Or if you just want the final hidden state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=Variable containing:\n",
       " 0.2458 -1.2492  1.7491  0.3977  0.8368 -2.9893  1.0737 -0.6264 -0.2431 -0.3737\n",
       "-0.9855 -0.7507  0.6510  0.8058 -0.7101  0.1943 -0.3969 -0.3953  1.6264  1.6685\n",
       " 1.1175 -1.7670  0.9485  1.0839  0.2394  0.0025 -1.3496 -0.7555 -0.4157 -0.1383\n",
       " 1.1401  1.2943 -0.1035  0.5639  0.8211 -1.2519 -1.8266  1.3199  1.3147  0.8338\n",
       "-0.3011  0.3611  0.5286 -0.9234  0.5133 -0.6587  0.8846 -0.4795  0.0756 -0.1951\n",
       " 0.7663 -0.7539 -1.0057  0.1264  0.1875  0.8970 -0.4165 -1.0588  0.0584  0.1958\n",
       " 1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058  1.4893\n",
       "-0.4987 -0.7722  0.8738  1.2237  0.2091 -1.1581 -1.6838 -1.4353  0.5427 -1.0765\n",
       "-0.9855 -0.7507  0.6510  0.8058 -0.7101  0.1943 -0.3969 -0.3953  1.6264  1.6685\n",
       " 1.3021  0.2287 -1.2955  0.5958 -0.5541  0.9566  0.3134  1.2365 -0.5294  1.2598\n",
       " 1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058  1.4893\n",
       " 0.3945  1.1748 -0.7199 -0.3348  1.3338 -0.6700  0.5455 -0.8923  0.3145  1.5411\n",
       " 0.2288  2.0369  1.1769 -0.5033 -1.4581  0.1204  0.6094 -0.1838  0.1945 -0.4567\n",
       " 0.6469 -0.2368 -0.6551  0.9555 -1.4819 -1.1243 -0.3334  0.7092  0.1606  1.3043\n",
       " 0.3073  0.1540  0.5178  0.9340  1.4234 -0.8974 -0.0834  0.7961 -1.6986 -0.2221\n",
       " 1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058  1.4893\n",
       " 0.3013  0.0212 -0.2998  0.6131 -1.3407 -0.1391 -0.6390  1.5347  1.4896 -1.0377\n",
       " 1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058  1.4893\n",
       "-0.6558 -0.3632  1.1079 -0.6666  0.8527 -0.6235 -0.9334 -0.6754 -0.1551 -0.9527\n",
       "-0.0253  0.6420  0.9749  0.3295  1.0226  1.1452  0.7207 -0.8234 -0.6984 -1.6346\n",
       " 1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058  1.4893\n",
       " 1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058  1.4893\n",
       " 0.3073  0.1540  0.5178  0.9340  1.4234 -0.8974 -0.0834  0.7961 -1.6986 -0.2221\n",
       "-0.6558 -0.3632  1.1079 -0.6666  0.8527 -0.6235 -0.9334 -0.6754 -0.1551 -0.9527\n",
       " 1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058  1.4893\n",
       "[torch.FloatTensor of size 25x10]\n",
       ", batch_sizes=[3, 3, 3, 3, 3, 3, 2, 2, 2, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058  1.4893\n",
       "-0.4987 -0.7722  0.8738  1.2237  0.2091 -1.1581 -1.6838 -1.4353  0.5427 -1.0765\n",
       "-0.9855 -0.7507  0.6510  0.8058 -0.7101  0.1943 -0.3969 -0.3953  1.6264  1.6685\n",
       "[torch.FloatTensor of size 3x10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor[2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.2458 -1.2492  1.7491  0.3977  0.8368 -2.9893  1.0737 -0.6264 -0.2431\n",
       "  -0.9855 -0.7507  0.6510  0.8058 -0.7101  0.1943 -0.3969 -0.3953  1.6264\n",
       "   1.1175 -1.7670  0.9485  1.0839  0.2394  0.0025 -1.3496 -0.7555 -0.4157\n",
       " \n",
       " Columns 9 to 9 \n",
       "   -0.3737\n",
       "   1.6685\n",
       "  -0.1383\n",
       " \n",
       " (1 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    1.1401  1.2943 -0.1035  0.5639  0.8211 -1.2519 -1.8266  1.3199  1.3147\n",
       "  -0.3011  0.3611  0.5286 -0.9234  0.5133 -0.6587  0.8846 -0.4795  0.0756\n",
       "   0.7663 -0.7539 -1.0057  0.1264  0.1875  0.8970 -0.4165 -1.0588  0.0584\n",
       " \n",
       " Columns 9 to 9 \n",
       "    0.8338\n",
       "  -0.1951\n",
       "   0.1958\n",
       " \n",
       " (2 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058\n",
       "  -0.4987 -0.7722  0.8738  1.2237  0.2091 -1.1581 -1.6838 -1.4353  0.5427\n",
       "  -0.9855 -0.7507  0.6510  0.8058 -0.7101  0.1943 -0.3969 -0.3953  1.6264\n",
       " \n",
       " Columns 9 to 9 \n",
       "    1.4893\n",
       "  -1.0765\n",
       "   1.6685\n",
       " \n",
       " (3 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    1.3021  0.2287 -1.2955  0.5958 -0.5541  0.9566  0.3134  1.2365 -0.5294\n",
       "   1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058\n",
       "   0.3945  1.1748 -0.7199 -0.3348  1.3338 -0.6700  0.5455 -0.8923  0.3145\n",
       " \n",
       " Columns 9 to 9 \n",
       "    1.2598\n",
       "   1.4893\n",
       "   1.5411\n",
       " \n",
       " (4 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.2288  2.0369  1.1769 -0.5033 -1.4581  0.1204  0.6094 -0.1838  0.1945\n",
       "   0.6469 -0.2368 -0.6551  0.9555 -1.4819 -1.1243 -0.3334  0.7092  0.1606\n",
       "   0.3073  0.1540  0.5178  0.9340  1.4234 -0.8974 -0.0834  0.7961 -1.6986\n",
       " \n",
       " Columns 9 to 9 \n",
       "   -0.4567\n",
       "   1.3043\n",
       "  -0.2221\n",
       " \n",
       " (5 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058\n",
       "   0.3013  0.0212 -0.2998  0.6131 -1.3407 -0.1391 -0.6390  1.5347  1.4896\n",
       "   1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058\n",
       " \n",
       " Columns 9 to 9 \n",
       "    1.4893\n",
       "  -1.0377\n",
       "   1.4893\n",
       " \n",
       " (6 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "   -0.6558 -0.3632  1.1079 -0.6666  0.8527 -0.6235 -0.9334 -0.6754 -0.1551\n",
       "  -0.0253  0.6420  0.9749  0.3295  1.0226  1.1452  0.7207 -0.8234 -0.6984\n",
       "  -1.8696  0.5178 -2.5933  0.2404 -0.2296  0.7743 -0.7963 -0.2847 -1.3640\n",
       " \n",
       " Columns 9 to 9 \n",
       "   -0.9527\n",
       "  -1.6346\n",
       "   1.0030\n",
       " \n",
       " (7 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058\n",
       "   1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058\n",
       "  -1.8696  0.5178 -2.5933  0.2404 -0.2296  0.7743 -0.7963 -0.2847 -1.3640\n",
       " \n",
       " Columns 9 to 9 \n",
       "    1.4893\n",
       "   1.4893\n",
       "   1.0030\n",
       " \n",
       " (8 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    0.3073  0.1540  0.5178  0.9340  1.4234 -0.8974 -0.0834  0.7961 -1.6986\n",
       "  -0.6558 -0.3632  1.1079 -0.6666  0.8527 -0.6235 -0.9334 -0.6754 -0.1551\n",
       "  -1.8696  0.5178 -2.5933  0.2404 -0.2296  0.7743 -0.7963 -0.2847 -1.3640\n",
       " \n",
       " Columns 9 to 9 \n",
       "   -0.2221\n",
       "  -0.9527\n",
       "   1.0030\n",
       " \n",
       " (9 ,.,.) = \n",
       " \n",
       " Columns 0 to 8 \n",
       "    1.1806  0.2788 -0.1120 -0.7458  0.6243  0.3237  0.7052  0.7941 -1.0058\n",
       "  -1.8696  0.5178 -2.5933  0.2404 -0.2296  0.7743 -0.7963 -0.2847 -1.3640\n",
       "  -1.8696  0.5178 -2.5933  0.2404 -0.2296  0.7743 -0.7963 -0.2847 -1.3640\n",
       " \n",
       " Columns 9 to 9 \n",
       "    1.4893\n",
       "   1.0030\n",
       "   1.0030\n",
       " [torch.FloatTensor of size 10x3x10], array([10,  9,  6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor, seq_lengths.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 3, 10]), \n",
       "  10\n",
       "   9\n",
       "   6\n",
       " [torch.LongTensor of size 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor.shape, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
