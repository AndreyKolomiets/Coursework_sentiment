{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "import os\n",
    "from gensim.models.word2vec import PathLineSentences, LineSentence\n",
    "import pandas as pd\n",
    "import imp\n",
    "from tqdm import tqdm_notebook\n",
    "import multiprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ak/Yandex.Disk.localized/sentiment-neural_past_from_Denis_Kirjanov/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer\n",
    "imp.reload(transformer)\n",
    "from nltk_sent_tokenize_binding import NLTKSentenceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовить тексты из чатов:\n",
    "* Выгрузить тексты\n",
    "* Выгрузить модель\n",
    "* Нормализовать\n",
    "* Выгрузить ранее нормализованные тексты\n",
    "* Объединить\n",
    "* Сохранить в один файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Я внес деньги на карту. Когда они придут, я их...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Кредитная</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>По карте</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>Да</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Да</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                                                  1\n",
       "0   2  Я внес деньги на карту. Когда они придут, я их...\n",
       "1   4                                          Кредитная\n",
       "2   6                                           По карте\n",
       "3   8                                                 Да\n",
       "4  10                                                 Да"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = '/Users/ak/Yandex.Disk.localized/Магистратура ВШЭ/Chat_word_autofill/'\n",
    "df = pd.read_csv(root_path + 'Client_messages_cleaned.csv', encoding='cp1251', delimiter=';', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_stoppings = [\"лат\", \"греч\", \"проц\", \"млн\", \"млрд\", \"итал\", \"фр\", \"рус\", \"кв\", \"м\", \"тыс\", \"руб\", \"англ\", \"п\",\n",
    "                       \"ст\", \"гор\", \"обл\", \"пос\", \"д\", \"дер\", \"др\", \"пр\", \"рег\", \"ул\", \"к\", \"т.е\", \"т.п\", \"юр\", \"физ\"]\n",
    "sentence_tokenizer = NLTKSentenceTokenizer(false_stoppings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd3cc4fcb4643e8b7549b4eda98a48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=819752), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for text in tqdm_notebook(df[1].values):\n",
    "    if type(text) in [str, object]:\n",
    "        for sentence in text.split('\\n'):\n",
    "            s = sentence_tokenizer(sentence)\n",
    "            sentences.extend(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028955"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model_tokenize.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def norm12345(s):\n",
    "    tokens = model.lemmatizer.process(s)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 880 ms, sys: 533 ms, total: 1.41 s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = multiprocessing.Pool(3)\n",
    "sent_norm = pool.map(norm12345, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_prev = pd.read_csv('sentences_normalized.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>рабочий место выделять рабочий место кредит касса</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos терминал сенсорный экран для управление сб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>очередь иметься</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>free wi fi tattelecom good while waiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>so pretty interior though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  рабочий место выделять рабочий место кредит касса\n",
       "1  pos терминал сенсорный экран для управление сб...\n",
       "2                                    очередь иметься\n",
       "3           free wi fi tattelecom good while waiting\n",
       "4                          so pretty interior though"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_norm.extend(sentences_prev[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1586241"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('sentences_normalized_with_chats.txt', sent_norm, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 µs, sys: 89 µs, total: 121 µs\n",
      "Wall time: 124 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ver = 2\n",
    "logging.basicConfig(filename='w2v_normv' + str(ver) + '_train.log', format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "\n",
    "# class StringsFeeder(object):\n",
    "#     def __init__(self, dirname):\n",
    "#         self.dirname = dirname\n",
    "#         self.files = [f for f in os.listdir(self.dirname) if os.path.join(self.dirname, f) and f.endswith('.txt')]\n",
    "#         print(self.files)\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         for fname in self.files:\n",
    "#             for line in open(os.path.join(self.dirname, fname), encoding='utf8'):\n",
    "#                 yield line.split()\n",
    "\n",
    "sentences = LineSentence(root_path + 'sentences_normalized_with_chats.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 19s, sys: 8.52 s, total: 16min 28s\n",
      "Wall time: 8min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = gensim.models.Word2Vec(sentences, min_count=20, size=300, workers=3, iter=40, window=4)\n",
    "model.save(root_path + '/model_normalized_with_chats_' + str(ver) + '.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/ak/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "emb = model.wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Vocab at 0x1a6f9b8b38>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab['спать']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    try:\n",
    "        vec = model.wv(word)\n",
    "        return vec\n",
    "    except KeyError:\n",
    "        return np.zeros(model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(size=(100, 6)) > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 1],\n",
       "       [1, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 1],\n",
       "       [1, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
